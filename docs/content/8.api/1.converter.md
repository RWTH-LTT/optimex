---
title: converter
description: API documentation for optimex.converter
---

# converter

Model input conversion and validation for optimization.

This module bridges LCA data processing and optimization by converting outputs from
LCADataProcessor into structured OptimizationModelInputs. It provides validation,
scaling, serialization, and constraint management for optimization model inputs.

Key classes:
    - OptimizationModelInputs: Validated data structure for optimization inputs
    - ModelInputManager: Handles conversion, serialization, and constraint overrides


## Classes

### `ModelInputManager`

Interface between LCA data processing and optimization modeling.

    The `ModelInputManager` is responsible for transforming, validating, and managing
    structured data inputs for optimization models derived from an `LCADataProcessor`.

    Responsibilities:

    - Extracts raw structural and quantitative data from an `LCADataProcessor` instance.
    - Constructs and validates a `OptimizationModelInputs` Pydantic model, ensuring all necessary
      fields are populated and internally consistent.
    - Allows for user-defined overrides of any input fields to enable customization,
      correction, or scenario-specific tuning.
    - Supports serialization and deserialization of `OptimizationModelInputs` for reproducibility,
      sharing, or caching via `.json` or `.pickle`.
    - Provides access to scaled versions of the model inputs (e.g., for numerical
      stability in optimization solvers), with metadata on scaling transformations.

    This class is intended to serve as the main interface between upstream life cycle
    assessment (LCA) data and downstream optimization workflows, abstracting away
    validation, preprocessing, and I/O concerns from both ends.

    Example
    -------
```python
    >>> # Initialize
    >>> manager = ModelInputManager()
    >>>
    >>> # Parse data from LCA data processor
    >>> inputs = manager.parse_from_lca_processor(lca_data_processor)
    >>>
    >>> # Optionally override fields
    >>> inputs = manager.override_inputs(PROCESS=["P1", "P2"], demand={...})
    >>>
    >>> # Save to disk
    >>> manager.save("inputs.json")
    >>>
    >>> # Load from disk
    >>> manager.load("inputs.json")
    >>>
    >>> # Get a numerically scaled version
    >>> scaled_inputs, scale_factors = inputs.get_scaled_copy()
```


#### Methods

##### `__init__(self)`

Initialize a new ModelInputManager with empty model inputs.

        The manager starts with no model inputs. Use `parse_from_lca_processor()`
        to populate inputs from an LCADataProcessor, or use `load()` to load
        previously saved inputs from disk.


##### `load(self, path: str) -> OptimizationModelInputs`

Load OptimizationModelInputs from a JSON or pickle file.


##### `override(self, **overrides) -> OptimizationModelInputs`

Override fields of the current OptimizationModelInputs instance and re-validate.

        Parameters:
            overrides: Keyword arguments matching OptimizationModelInputs fields to override.


##### `parse_from_lca_processor(self, lca_processor: LCADataProcessor) -> OptimizationModelInputs`

Extracts data from the LCADataProcessor and constructs OptimizationModelInputs.


##### `save(self, path: str) -> None`

Save the current OptimizationModelInputs to a JSON or pickle file based on extension.
        Supports .json and .pkl extensions.


### `OptimizationModelInputs`

Interface data structure for linking LCA-based outputs with optimization inputs.

    This class organizes all relevant inputs needed to build a temporal, process-based
    life cycle model suitable for linear optimization, including foreground and
    background exchanges, temporal system information, and optional process constraints.


#### Methods

##### `check_all_keys(cls, data)`

Validate that all dictionary keys reference valid set elements.

        This validator ensures that all keys in the input dictionaries (e.g., demand,
        foreground_technosphere) reference elements that exist in the corresponding
        sets (e.g., PROCESS, PRODUCT, SYSTEM_TIME). This prevents runtime errors
        from invalid references.

        Parameters
        ----------
        data : dict
            The raw data dictionary before model instantiation.

        Returns
        -------
        dict
            The validated data dictionary.

        Raises
        ------
        ValueError
            If any dictionary key references an element not in the corresponding set.


##### `get_scaled_copy(self) -> Tuple['OptimizationModelInputs', Dict[str, Any]]`

Create a scaled copy of inputs for numerical stability in optimization.

        Scaling improves solver performance by normalizing values to similar magnitudes.
        The method scales foreground tensors, characterization factors, demand, and
        limits while preserving the original data structure. Scaling factors are returned
        for denormalizing results.

        Returns
        -------
        tuple[OptimizationModelInputs, dict]
            - Scaled copy of the model inputs
            - Dictionary of scaling factors used:
                - "foreground": Scale factor for all foreground tensors and demand
                - "characterization": Dict mapping each category to its scale factor


##### `validate_constant_operation_flows(self) -> 'OptimizationModelInputs'`

Validate that flows marked as operational are constant over process time.

        For flexible operation mode, flows that occur during the operation phase must
        have constant values across process time steps. This is because the optimization
        scales these flows linearly with the operation variable. Time-varying operational
        flows would require fixed operation mode instead.

        Returns
        -------
        OptimizationModelInputs
            Self, after validation.

        Raises
        ------
        ValueError
            If any operational flow has varying values across process time.


##### `validate_process_limits_consistency(self) -> 'OptimizationModelInputs'`

Validate that min limits are not greater than max limits for process bounds.

        This ensures logical consistency of the bounds - having min > max would
        create an infeasible constraint.


##### `warn_negative_tau_boundary(self) -> 'OptimizationModelInputs'`

Warn about negative process times that may fall outside SYSTEM_TIME.

        When tau < 0 (e.g., construction before deployment), the contribution
        appears at system time (t - tau). If min(SYSTEM_TIME) - tau < min(SYSTEM_TIME),
        those contributions are lost for early installations.

        Example: With SYSTEM_TIME starting at 2020 and tau=-1:
        - Installation at 2020 has construction at t=2019 (NOT in SYSTEM_TIME)
        - These emissions are silently ignored

        This validator warns users about this boundary condition.

